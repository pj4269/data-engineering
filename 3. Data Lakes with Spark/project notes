import configparser
from datetime import datetime
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format
import pyspark.sql.functions as F


# using SQLContext to read parquet file
from pyspark.sql import SQLContext

config = configparser.ConfigParser()
config.read('dl.cfg')
#os.environ['AWS_ACCESS_KEY_ID']=config['AWS_ACCESS_KEY_ID']
#os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS_SECRET_ACCESS_KEY']


#config = configparser.ConfigParser()
#Normally this file should be in ~/.aws/credentials
#config.read_file(open('aws/credentials.cfg'))
os.environ["AWS_ACCESS_KEY_ID"]= config['AWS']['AWS_ACCESS_KEY_ID']
os.environ["AWS_SECRET_ACCESS_KEY"]= config['AWS']['AWS_SECRET_ACCESS_KEY']


def create_spark_session():
    spark = SparkSession \
        .builder \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:2.7.0") \
        .getOrCreate()
    return spark

# df = spark.read.csv("s3a://udacity-dend/pagila/payment/payment.csv")

def process_song_data(spark, input_data, output_data):
    
    # get filepath to song data file
    #song_data1 = input_data + '/song_data/A/B/C/TRABCEI128F424C983.json'
    song_data = input_data + '/song_data/A/B/C/*.json'
    #song_data2 = input_data + "/song_data/*/*/*/*.json"

    # read song data file
    
    df = spark.read.json(song_data)
    df.printSchema()
    df.show(5)
    # extract columns to create songs table
    '''
    song_table = df.select('song_id', 'title', 'artist_id', 'year', 'duration')
    song_table = song_table.toPandas()
    print (song_table.head() )
    '''
    
    # write songs table to parquet files partitioned by year and artist: done
    #songs_table  
    #df.select('song_id', 'title', 'artist_id', 'year',
    #          'duration').write.partitionBy("year","artist_id").parquet("song_table.parquet")
    
    # extract columns to create artists table
    artists_table = df.select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude')
    artists_table = artists_table.toPandas()
    print (artists_table.head() )
    
    # write artists table to parquet files
    #artists_table
    df.select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 
              'artist_longitude').write.parquet("artist_table.parquet")


def process_log_data(spark, input_data, output_data):
    # get filepath to log data file
    log_data = input_data + '/log_data/2018/11/2018-11-12-events.json'
    #log_data = input_data + '/log_data/2018/11/*.json' 
    

    # read log data file
    df = spark.read.json(log_data)
    #df.printSchema()
    #df.show(5)
    
    # filter by actions for song plays # ?
    #df = df.filter(page = 'NextSong')
    df = df.where("page= 'NextSong'")
    df.printSchema()

    #df.show(5)
    '''
    # extract columns for users table: done    
    users_table = df.select('userId', 'firstName', 'lastName', 'gender', 'level')
    users_table = users_table.toPandas()
    print (users_table.head() )

    
    # write users table to parquet files
    #users_table
    df.select('userId', 'firstName', 'lastName', 'gender', 'level').write.parquet("users_table.parquet")
    '''
    # create timestamp column from original timestamp column
    
    #get_timestamp = udf()
    '''
    @udf
    def get_timestamp(col):
        import  pyspark.sql.functions as F        
        return F.to_timestamp(col)
    
    #square_udf_int = udf(lambda z: square(z), IntegerType())

    #df = 
    
    df1 = df.withColumn("timestamp", F.to_timestamp("ts"))

    df1.printSchema()
    df1.show(5)
    

    
    # create datetime column from original timestamp column
    get_datetime = udf()
    @udf
    def get_datetime(col):
         return F.to_datetime(col)
    
    from pyspark.sql.types import DateType
    
    #from pyspark.sql.types import DoubleType
    #df1 = df.withColumn("datetime",to_date("ts"))# not working
    #df2 = df1.withColumn("datetime", df1.timestamp.cast(DateType()))# having trouble
    df1 = df.withColumn('datetime', df.ts.cast('timestamp')) # working
  
    #df = 
    
    # extract columns to create time table
    # start_time, hour, day, week, month, year, weekday
    
    
    from pyspark.sql.functions import col
    time_table = df1.select(year("datetime").alias('year'), month("datetime").alias('month'), dayofyear("datetime").alias('day'), weekofyear("datetime").alias('week'), dayofweek("datetime").alias('weekday'),hour("datetime").alias('hour'), col("datetime").alias('start_time') )
    
    #time_table = 
    #time_table = df.select('userId', 'firstName', 'lastName', 'gender', 'level')
    #time_table = time_table.toPandas() # not working: year 5000 out of range error
    #print (time_table.head() )
    
    
    # write time table to parquet files partitioned by year and month
    #time_table
    time_table.write.partitionBy("year","month").parquet("time_table.parquet")
    '''
    # read in song data to use for songplays table
    sc = spark.sparkContext
    sqlContext = SQLContext(sc)
    # to read parquet file
    #df = sqlContext.read.parquet('path-to-file/commentClusters.parquet')
    '''
    artist_table = sqlContext.read.parquet('artist_table.parquet')
    song_table = sqlContext.read.parquet('song_table.parquet')
    time_table = sqlContext.read.parquet('time_table.parquet')
    users_table = sqlContext.read.parquet('users_table.parquet')
    artist_table.show(5)
    # version 1:
    artist_table.createOrReplaceTempView("artist_table")
    song_table.createOrReplaceTempView("song_table")
    time_table.createOrReplaceTempView("time_table")
    users_table.createOrReplaceTempView("users_table")
    '''
    # here
    # version 2: => song_data and log_data join on duration =length ?, song_data.title = df.song
    # artist_name = artist

    #song_data = input_data + '/song_data/A/B/*/*.json' 
    song_data = input_data + '/song_data/A/B/C/TRABCEI128F424C983.json'
    song_data = spark.read.json(song_data)
    # df
    # df = df1.join(df2, (df1.x1 == df2.x1) & (df1.x2 == df2.x2))

    inner_join = df.join(song_data, (df.artist == song_data.artist_name) & (df.length == song_data.duration) & (song_data.title == df.song) )
    #inner_join.show(5)
    #inner_join.createOrReplaceTempView("songplays_table")
    
    #inner_join.select(songplay_id, start_time, userId user_id, level, song_id, artist_id, session_id, location, user_agent

    
    songplay_table = inner_join.select( col("ts").alias('start_time') , col("userId").alias('user_id'), 'level', 'song_id', 'artist_id','sessionId', 'location', 'userAgent' )
    songplay_table.show(5)   
    #songplay_table.schema.fields
                                  
    # new from here
    
    from pyspark.sql import Row
    from pyspark.sql.types import StructField, StructType, LongType

    #row = Row("foo", "bar")
    row_with_index = Row(*["songplay_id"] + songplay_table.columns)

    #df = sc.parallelize([row("a", -1.0), row("b", -2.0), row("c", -3.0)]).toDF()

    def make_row(columns):
        def _make_row(row, uid):
            row_dict = row.asDict()
            return row_with_index(*[uid] + [row_dict.get(c) for c in columns])
        return _make_row

    f = make_row(songplay_table.columns)

    songplays = (songplay_table.rdd.zipWithUniqueId().map(lambda x: f(*x)).toDF(StructType([StructField("songplay_id", LongType(), False)] + songplay_table.schema.fields)))
    songplays.show(5)

    '''
    test = spark.sql(""" SELECT *
    FROM artist_table
    """)
    test.show(5)
    sqlContext.sql("CREATE TABLE IF NOT EXISTS songplay (key INT, value STRING)")
    '''
    # songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
    #song_df = 

    # extract columns from joined song and log datasets to create songplays table 
    #songplays_table = 

    # write songplays table to parquet files partitioned by year and month
    #songplays_table
    songplays = songplays.withColumn("start_time", F.to_timestamp("start_time"))
    #songplays.withColumn('start_time', songplays.start_time.cast('timestamp')).show(5) # working
    # works
    songplays = songplays.select(songplays.columns + [year("start_time").alias('year'), month("start_time").alias('month') ])
    songplays.select(songplays.columns).write.partitionBy('year','month' ).select(songplays.columns).parquet("songplays_test.parquet")
    songplays.show(5)
    
    #songplays.select(songplays.columns).show(5)
    #songplays = songplays.select(year("datetime").alias('year'), month("datetime").alias('month'), dayofyear("datetime").alias('day'), weekofyear("datetime").alias('week'), dayofweek("datetime").alias('weekday'),hour("datetime").alias('hour'), col("datetime").alias('start_time') )

    #songplays.select(year("start_time").alias(),month("start_time") ).show(5)

    #songplays.write.parquet(output_data + "songplays.parquet")
    # songs_table.write.parquet(output + "song_table")

    #df = spark.read.csv("s3a://udacity-dend/pagila/payment/payment.csv")


def main():
    
    spark = create_spark_session()
    input_data = "s3a://udacity-dend/"
    output_data = "s3a://udacity-dend/" #should be same as input!
    
    
    #process_song_data(spark, input_data, output_data)    
    process_log_data(spark, input_data, output_data)


if __name__ == "__main__":
    main()

'''
    songplay_table_insert = ("INSERT into songplays\
(start_time, user_id, level, song_id, artist_id, sessionId, location, user_agent)\
    SELECT\
    ts, user_id, level, song_id, artist_id, sessionId, location, user_agent\
    FROM\
    (SELECT e.ts, e.user_id, e.level, a.song_id, a.artist_id, e.sessionId, e.location, e.user_agent\
    FROM \
    staging_events e\
    JOIN\
    (select song.song_id, artist.artist_id, song.title, artist.name, song.duration FROM songs song \
    JOIN \
    artists artist \
    ON \
    song.artist_id = artist.artist_id) AS a \
    ON \
    (a.title = e.song \
    AND a.name = e.artist \
    AND a.duration = e.length) \
    WHERE e.page='nextsong' \
    AND user_id is not null)  ")
'''
